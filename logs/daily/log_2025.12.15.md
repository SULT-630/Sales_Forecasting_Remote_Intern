# Daily Log â€” 2025-12-15

## Objective
- Config git and github to manage code and versions
- Setup of the whole project (environment, structure and etc.)
- Dataset import and first glance at data cleaning and pre-processing

## Work Done
- Git repo created and ssh-connection established
- Structure decided and Create .gitignore, .venv, .env and requirements.txt
- Validation of entry and connection to src
- Examples of README, logs and code are added to ensure formality
- Import dataset from Kaggle and download train_*.csv into local dir

## Key Findings
- xxx
- xxx

## Issues Encountered
- Several runs of 01_load_data.py may lead to residual old csv files
- There's data whose is_featured_sku and is_displayed sku are 0 while its total_price is still smaller than its base_price
- Connection between src and scripts failed due to unknown reasons

## How Issues Were Resolved
- Define a cleaning function to clear out RAW_PATH first
- Solved by adding a pyproject.toml and pip install e .

## Problems
- Confused by usage of git branch - when to use and whether I should use in an individual project
- Whether the procedure is enough for industry-level production or is too complicated
- In the process of dataloading, why the string object of path must be tranferred into PATH object
- Why we use parquet instead of csv and whether parquet is used more often in industry environment or it's csv
- Why record_IDs are not continuous and miss some lines and why there's data with week start date at 2030/5/11. And there's a larger time gap than the description stated as 3 years only
- How should we go through Issues Encountered.2

## Next Steps
- First glance at data analysis with visualizations and Implementations of cleaning
- Spatial sequence recognition